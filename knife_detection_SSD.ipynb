{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "knife_detection_SSD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbjaLsAwoRN8"
      },
      "source": [
        "## Choosing a pre-training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmvizC-uSwbT"
      },
      "source": [
        "# Some available models to train on\n",
        "MODELS_CONFIG = {\n",
        "    'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "    },\n",
        "    'ssd_inception_v2': {\n",
        "        'model_name': 'ssd_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'ssd_inception_v2_coco.config',\n",
        "    },\n",
        "    'ssd_resnet_50_fpn_coco': {\n",
        "        'model_name': 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03',\n",
        "        'pipeline_file': 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config',\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
        "    },\n",
        "    'rfcn_resnet101': {\n",
        "        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n",
        "        'pipeline_file': 'rfcn_resnet101_pets.config',\n",
        "    }\n",
        "}\n",
        "\n",
        "selected_model = 'ssd_inception_v2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4vOwO6mo5ny"
      },
      "source": [
        "## Installing Required Packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9seFxjwHUGP7"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0 #downgrade\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "\n",
        "!pip install -qq Cython contextlib2 pillow lxml matplotlib\n",
        "\n",
        "!pip install -qq pycocotools\n",
        "\n",
        "!pip install tf_slim\n",
        "\n",
        "!pip install lvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJlFeKrVpXb7"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KDqwpzDXpeBS"
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "import tensorflow.compat.v1 as tf\n",
        "import os\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import csv\n",
        "import cv2\n",
        "\n",
        "from collections import namedtuple, OrderedDict\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O79ZBY-TU60L"
      },
      "source": [
        "## Mounting to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEjSCv9Fpmcu"
      },
      "source": [
        "from google.colab import drive\n",
        " \n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/'My Drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_DDk9JirFzo"
      },
      "source": [
        "## Downloading and Organizing Images and Annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eN7GS8hrG3l"
      },
      "source": [
        "#Project Directory\n",
        "!mkdir knife_detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1RLyhq4rZhk"
      },
      "source": [
        "cd knife_detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEZAeDcmrzAr"
      },
      "source": [
        "!git clone https://github.com/ari-dasci/OD-WeaponDetection.git "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF8LOVx4r0du"
      },
      "source": [
        "!mkdir data\n",
        "\n",
        "!mkdir data/images data/train_labels data/test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6roin3Er9Lk"
      },
      "source": [
        "!mv OD-WeaponDetection/Knife_detection/Images/* data/images\n",
        "!mv OD-WeaponDetection/Knife_detection/annotations/* data/train_labels\n",
        "\n",
        "# Label first 400 image as testing\n",
        "!ls data/train_labels/* | sort -R | head -400 | xargs -I{} mv {} data/test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RIlaHSHsV0h"
      },
      "source": [
        "!ls -1 data/train_labels/ | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn8mYFb8sWTm"
      },
      "source": [
        "!ls -1 data/test_labels/ | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYVVzHYt2fQ"
      },
      "source": [
        "## Preprocessing Images and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukhCYn-yt3bB"
      },
      "source": [
        "%cd /gdrive/My Drive/knife_detection/data\n",
        "\n",
        "images_extension = 'jpg'\n",
        "\n",
        "def xml_to_csv(path):\n",
        "  classes_names = []\n",
        "  xml_list = []\n",
        "\n",
        "  for xml_file in glob.glob(path + '/*.xml'):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    for member in root.findall('object'):\n",
        "      classes_names.append(member[0].text)\n",
        "      value = (root.find('filename').text,\n",
        "               int(root.find('size')[0].text),\n",
        "               int(root.find('size')[1].text),\n",
        "               member[0].text,\n",
        "               int(member[4][0].text),\n",
        "               int(member[4][1].text),\n",
        "               int(member[4][2].text),\n",
        "               int(member[4][3].text))\n",
        "      xml_list.append(value)\n",
        "  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "  xml_df = pd.DataFrame(xml_list, columns=column_name) \n",
        "  classes_names = list(set(classes_names))\n",
        "  classes_names.sort()\n",
        "  return xml_df, classes_names\n",
        "\n",
        "for label_path in ['train_labels', 'test_labels']:\n",
        "  image_path = os.path.join(os.getcwd(), label_path)\n",
        "  xml_df, classes = xml_to_csv(label_path)\n",
        "  xml_df.to_csv(f'{label_path}.csv', index=None)\n",
        "  print(f'Successfully converted {label_path} xml to csv.')\n",
        "\n",
        "label_map_path = os.path.join(\"label_map.pbtxt\")\n",
        "\n",
        "pbtxt_content = \"\"\n",
        "\n",
        "for i, class_name in enumerate(classes):\n",
        "    pbtxt_content = (\n",
        "        pbtxt_content\n",
        "        + \"item {{\\n    id: {0}\\n    name: '{1}'\\n    display_name: 'Knife'\\n }}\\n\\n\".format(i + 1, class_name)\n",
        "    )\n",
        "pbtxt_content = pbtxt_content.strip()\n",
        "with open(label_map_path, \"w\") as f:\n",
        "    f.write(pbtxt_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3wYz6YmuQ4Q"
      },
      "source": [
        "!cat label_map.pbtxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e09_GB_8uaGk"
      },
      "source": [
        "%cd /gdrive/My Drive/knife_detection/data\n",
        "\n",
        "images_path = 'images'\n",
        "\n",
        "for CSV_FILE in ['train_labels.csv', 'test_labels.csv']:\n",
        "  with open(CSV_FILE, 'r') as fid:  \n",
        "      print('[*] Checking file:', CSV_FILE) \n",
        "      file = csv.reader(fid, delimiter=',')\n",
        "      first = True \n",
        "      cnt = 0\n",
        "      error_cnt = 0\n",
        "      error = False\n",
        "      for row in file:\n",
        "          if error == True:\n",
        "              error_cnt += 1\n",
        "              error = False         \n",
        "          if first == True:\n",
        "              first = False\n",
        "              continue     \n",
        "          cnt += 1      \n",
        "          name, width, height, xmin, ymin, xmax, ymax = row[0], int(row[1]), int(row[2]), int(row[4]), int(row[5]), int(row[6]), int(row[7])     \n",
        "          path = os.path.join(images_path, name)\n",
        "          img = cv2.imread(path)         \n",
        "          if type(img) == type(None):\n",
        "              error = True\n",
        "              print('Could not read image', img)\n",
        "              continue     \n",
        "          org_height, org_width = img.shape[:2]     \n",
        "          if org_width != width:\n",
        "              error = True\n",
        "              print('Width mismatch for image: ', name, width, '!=', org_width)     \n",
        "          if org_height != height:\n",
        "              error = True\n",
        "              print('Height mismatch for image: ', name, height, '!=', org_height) \n",
        "          if xmin > org_width:\n",
        "              error = True\n",
        "              print('XMIN > org_width for file', name)  \n",
        "          if xmax > org_width:\n",
        "              error = True\n",
        "              print('XMAX > org_width for file', name)\n",
        "          if ymin > org_height:\n",
        "              error = True\n",
        "              print('YMIN > org_height for file', name)\n",
        "          if ymax > org_height:\n",
        "              error = True\n",
        "              print('YMAX > org_height for file', name)\n",
        "          if error == True:\n",
        "              print('Error for file: %s' % name)\n",
        "              print()\n",
        "      print()\n",
        "      print('Checked %d files and realized %d errors' % (cnt, error_cnt))\n",
        "      print(\"-----\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSZ13olfx0-6"
      },
      "source": [
        "## Downloading Tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJgefA_Rx2Lf"
      },
      "source": [
        "# Downloads Tensorflow\n",
        "%cd /gdrive/My Drive/knife_detection\n",
        "!git clone --q https://github.com/tensorflow/models.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8C7CsimzEtP"
      },
      "source": [
        "%cd /gdrive/My Drive/knife_detection/models/research\n",
        "\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "os.environ['PYTHONPATH'] += ':/gdrive/My Drive/knife_detection/models/research/:/gdrive/My Drive/knife_detection/models/research/slim/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QNyBfkFzHgE"
      },
      "source": [
        "!python3 object_detection/builders/model_builder_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76o92WxBzenG"
      },
      "source": [
        "## Create Tf record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Ifr3bX1FhE"
      },
      "source": [
        "from object_detection.utils import dataset_util\n",
        "%cd /gdrive/My Drive/knife_detection/models/\n",
        "\n",
        "DATA_BASE_PATH = '/gdrive/My Drive/knife_detection/data/'\n",
        "image_dir = DATA_BASE_PATH +'images/'\n",
        "\n",
        "def class_text_to_int(row_label):\n",
        "\t\tif row_label == 'knife':\n",
        "\t\t\t\treturn 1\n",
        "\t\telse:\n",
        "\t\t\t\tNone\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "\t\tdata = namedtuple('data', ['filename', 'object'])\n",
        "\t\tgb = df.groupby(group)\n",
        "\t\treturn [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "\t\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "\t\t\t\tencoded_jpg = fid.read()\n",
        "\t\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "\t\timage = Image.open(encoded_jpg_io)\n",
        "\t\twidth, height = image.size\n",
        "\n",
        "\t\tfilename = group.filename.encode('utf8')\n",
        "\t\timage_format = b'jpg'\n",
        "\t\txmins = []\n",
        "\t\txmaxs = []\n",
        "\t\tymins = []\n",
        "\t\tymaxs = []\n",
        "\t\tclasses_text = []\n",
        "\t\tclasses = []\n",
        "\n",
        "\t\tfor index, row in group.object.iterrows():\n",
        "\t\t\t\txmins.append(row['xmin'] / width)\n",
        "\t\t\t\txmaxs.append(row['xmax'] / width)\n",
        "\t\t\t\tymins.append(row['ymin'] / height)\n",
        "\t\t\t\tymaxs.append(row['ymax'] / height)\n",
        "\t\t\t\tclasses_text.append(row['class'].encode('utf8'))\n",
        "\t\t\t\tclasses.append(class_text_to_int(row['class']))\n",
        "\n",
        "\t\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "\t\t\t\t'image/height': dataset_util.int64_feature(height),\n",
        "\t\t\t\t'image/width': dataset_util.int64_feature(width),\n",
        "\t\t\t\t'image/filename': dataset_util.bytes_feature(filename),\n",
        "\t\t\t\t'image/source_id': dataset_util.bytes_feature(filename),\n",
        "\t\t\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "\t\t\t\t'image/format': dataset_util.bytes_feature(image_format),\n",
        "\t\t\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "\t\t\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "\t\t\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "\t\t\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "\t\t\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "\t\t\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "\t\t}))\n",
        "\t\treturn tf_example\n",
        "\n",
        "for csv in ['train_labels', 'test_labels']:\n",
        "  writer = tf.io.TFRecordWriter(DATA_BASE_PATH + csv + '.record')\n",
        "  path = os.path.join(image_dir)\n",
        "  examples = pd.read_csv(DATA_BASE_PATH + csv + '.csv')\n",
        "  grouped = split(examples, 'filename')\n",
        "  for group in grouped:\n",
        "      tf_example = create_tf_example(group, path)\n",
        "      writer.write(tf_example.SerializeToString())\n",
        "    \n",
        "  writer.close()\n",
        "  output_path = os.path.join(os.getcwd(), DATA_BASE_PATH + csv + '.record')\n",
        "  print('Successfully created the TFRecords: {}'.format(DATA_BASE_PATH +csv + '.record'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHNGqkvg1Gbr"
      },
      "source": [
        "# TFRecords are created\n",
        "%cd /gdrive/My Drive/knife_detection/data\n",
        "!ls -lX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSRc4b0N1xs7"
      },
      "source": [
        "## Downloading the Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZadAeef110-H"
      },
      "source": [
        "%cd /gdrive/My Drive/knife_detection/models/\n",
        "\n",
        "DATA_BASE_PATH = '/gdrive/My Drive/knife_detection/data/'\n",
        "image_dir = DATA_BASE_PATH +'images/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgAtcTU_2FT2"
      },
      "source": [
        "%cd /gdrive/My Drive/knife_detection/models/research/\n",
        "!mkdir pretrained_model\n",
        "%cd pretrained_model\n",
        "!wget http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz\n",
        "!tar -xzvf ssd_inception_v2_coco_2018_01_28.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msheb1cw2Hip"
      },
      "source": [
        "%cd /gdrive/My Drive/knife_detection/models/research/pretrained_model\n",
        "\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "#the distination folder where the model will be saved\n",
        "fine_tune_dir = '/gdrive/My Drive/knife_detection/models/research/pretrained_model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW_xgEoK2JVc"
      },
      "source": [
        "!echo {fine_tune_dir}\n",
        "%cd {fine_tune_dir}\n",
        "!ls -alh "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTFy3R3w2mgT"
      },
      "source": [
        "## Configuring Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FdeSc142qit"
      },
      "source": [
        "CONFIG_BASE = \"/gdrive/My\\ Drive/knife_detection/models/research/object_detection/samples/configs\"\n",
        "\n",
        "#path to the specified model's config file\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "model_pipline = os.path.join(CONFIG_BASE, pipeline_file)\n",
        "model_pipline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t3eyzpH29lZ"
      },
      "source": [
        "%cd /gdrive/My\\ Drive/knife_detection/models/research/object_detection/samples/configs/\n",
        "!cat ssd_inception_v2_coco.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFYXykFw3BjB"
      },
      "source": [
        "#editing the configuration file to add the path for the TFRecords files, pbtxt,batch_size,num_steps,num_classes.\n",
        "# any image augmentation, hyperparemeter tunning (drop out, batch normalization... etc) would be editted here\n",
        "\n",
        "%%writefile ssd_inception_v2_coco.config\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1 # number of classes to be detected\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "        reduce_boxes_in_lowest_layer: true\n",
        "      }\n",
        "    }\n",
        "    # all images will be resized to the below W x H.\n",
        "    image_resizer { \n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        #use_dropout: false\n",
        "        use_dropout: true # to counter over fitting. you can also try tweaking its probability below\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 1\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "            # weight: 0.00004\n",
        "            weight: 0.001 # higher regularizition to counter overfitting\n",
        "          }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "          # batch_norm {\n",
        "          #   train: true,\n",
        "          #   scale: true,\n",
        "          #   center: true,\n",
        "          #   decay: 0.9997,\n",
        "          #   epsilon: 0.001,\n",
        "          # }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_inception_v2'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            # weight: 0.00004\n",
        "            weight: 0.001 # higher regularizition to counter overfitting\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.03\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.001,\n",
        "        }\n",
        "      }\n",
        "      override_base_feature_extractor_hyperparams: true\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "            anchorwise_output: true\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "            anchorwise_output: true\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000 \n",
        "        iou_threshold: 0.95\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 3\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        \n",
        "        max_detections_per_class: 16\n",
        "        max_total_detections: 16\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 16 # training batch size\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.003\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "\n",
        "  fine_tune_checkpoint: \"/gdrive/My Drive/knife_detection/models/research/pretrained_model/model.ckpt\" #the path to the pretrained model. \n",
        "  fine_tune_checkpoint_type:  \"detection\"\n",
        "  num_steps: 2000000 \n",
        "  \n",
        "\n",
        "  #data augmentaion is done here, you can remove or add more.\n",
        "  # They will help the model generalize but the training time will increase greatly by using more data augmentation.\n",
        "  # Check this link to add more image augmentation: https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto\n",
        "  \n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    random_adjust_contrast {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    #path to the training TFRecord\n",
        "    input_path: \"/gdrive/My Drive/knife_detection/data/train_labels.record\"\n",
        "  }\n",
        "  #path to the label map \n",
        "  label_map_path: \"/gdrive/My Drive/knife_detection/data/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  # the number of images in your \"testing\" data (was 600 but we removed one above :) )\n",
        "  num_examples: 400\n",
        "  # the number of images to display in Tensorboard while training\n",
        "  num_visualizations: 20\n",
        "\n",
        "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
        "  # Remove the below line to evaluate indefinitely.\n",
        "  #max_evals: 10\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "      \n",
        "    #path to the testing TFRecord\n",
        "    input_path: \"/gdrive/My Drive/knife_detection/data/test_labels.record\"\n",
        "  }\n",
        "  #path to the label map \n",
        "  label_map_path: \"/gdrive/My Drive/knife_detection/data/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRMTDm7b3Ghi"
      },
      "source": [
        "# where the model will be saved at each checkpoint while training \n",
        "model_dir = 'training/'\n",
        "\n",
        "!rm -rf {model_dir}\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t94aQw_K3vRn"
      },
      "source": [
        "# Downloading and Configuring Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxQ8MXAu33Xz"
      },
      "source": [
        "#downlaoding ngrok to be able to access tensorboard on google colab\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u98AmUxl35oB"
      },
      "source": [
        "#the logs that are created while training \n",
        "LOG_DIR = '/gdrive/My\\ Drive/knife_detection/models/research/object_detection/training/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW3ZO_QQ38js"
      },
      "source": [
        "#The link to tensorboard.\n",
        "#works after the training starts.\n",
        "\n",
        "### note: if you didnt get a link as output, rerun this cell and the one above\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViVHBtDt3-7E"
      },
      "source": [
        "# !saved_model_cli show --dir /gdrive/My\\ Drive/knife_detection/models/research/object_detection/training/export/Servo/1598887811 \\\n",
        "#   --tag_set serve --signature_def serving_default\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /gdrive/My\\ Drive/knife_detection/models/research/object_detection/training/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iadubhaW4LG_"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IngoDWMj4Oyi"
      },
      "source": [
        "%cd /gdrive/My\\ Drive/knife_detection/models/research/object_detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNuv8tdW4Upd"
      },
      "source": [
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "print(tf.__version__)\n",
        "tf.test.gpu_device_name()\n",
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6xlA0qz4eaW"
      },
      "source": [
        "!python3 model_main.py \\\n",
        "    --pipeline_config_path={model_pipline}\\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}